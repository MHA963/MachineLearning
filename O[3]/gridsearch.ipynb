{"cells":[{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212},"id":"_WTlJ5IcFE-w","executionInfo":{"status":"ok","timestamp":1700221290356,"user_tz":-60,"elapsed":107512,"user":{"displayName":"Hashem abdou","userId":"07504651210053120960"}},"outputId":"226d4222-0aed-43a9-c9da-e0628bea0dfc"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-c26ee92c-3a33-4ec2-ba08-146e4bff0180\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-c26ee92c-3a33-4ec2-ba08-146e4bff0180\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving __init__.py to __init__.py\n","Saving dataloaders.py to dataloaders.py\n","Saving kernelfuns.py to kernelfuns.py\n","Saving utils.py to utils.py\n","Saving versions.py to versions.py\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append(\"/content/libitmal\")"],"metadata":{"id":"u3lW9lPsFvtN","executionInfo":{"status":"ok","timestamp":1700221362028,"user_tz":-60,"elapsed":255,"user":{"displayName":"Hashem abdou","userId":"07504651210053120960"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rjzgnLA8idK-"},"source":["# SWMAL Exercise\n","\n","\n","## Hyperparameters and Gridsearch\n","\n","When instantiating a Scikit-learn model in python most or all constructor parameters have _default_ values. These values are not part of the internal model and are hence called ___hyperparameters___---in contrast to _normal_ model parameters, for example the neuron weights, $\\mathbf w$, for an `MLP` model.\n","\n","### Manual Tuning Hyperparameters\n","\n","Below is an example of the python constructor for the support-vector classifier `sklearn.svm.SVC`, with say the `kernel` hyperparameter having the default value `'rbf'`. If you should choose, what would you set it to other than `'rbf'`?\n","\n","```python\n","class sklearn.svm.SVC(\n","    C=1.0,\n","    kernel=’rbf’,\n","    degree=3,\n","    gamma=’auto_deprecated’,\n","    coef0=0.0,\n","    shrinking=True,\n","    probability=False,\n","    tol=0.001,\n","    cache_size=200,\n","    class_weight=None,\n","    verbose=False,\n","    max_iter=-1,\n","    decision_function_shape=’ovr’,\n","    random_state=None\n","  )\n","```  \n","\n","The default values might be a sensible general starting point, but for your data, you might want to optimize the hyperparameters to yield a better result.\n","\n","To be able to set `kernel` to a sensible value you need to go into the documentation for the `SVC` and understand what the kernel parameter represents, and what values it can be set to, and you need to understand the consequences of setting `kernel` to something different than the default...and the story repeats for every other hyperparameter!\n","\n","### Brute Force  Search\n","\n","An alternative to this structured, but time-consuming approach, is just to __brute-force__ a search of interesting hyperparameters, and  choose the 'best' parameters according to a fit-predict and some score, say 'f1'.\n","\n","<img src=\"https://itundervisning.ase.au.dk/SWMAL/L09/Figs/gridsearch.png\"  alt=\"WARNING: could not get image from server.\"  style=\"width:350px\">\n","<small><em>\n","    <center> Conceptual graphical view of grid search for two distinct hyperparameters. </center>\n","    <center> Notice that you would normally search hyperparameters like `alpha` with an exponential range, say [0.01, 0.1, 1, 10] or similar.</center>\n","</em></small>\n","\n","Now, you just pick out some hyperparameters, that you figure are important, set them to a suitable range, say\n","\n","```python\n","    'kernel':('linear', 'rbf'),\n","    'C':[1, 10]\n","```\n","and fire up a full (grid) search on this hyperparameter set, that will try out all your specified combination of `kernel` and `C` for the model, and then prints the hyperparameter set with the highest score...\n","\n","The demo code below sets up some of our well known 'hello-world' data and then run a _grid search_ on a particular model, here a _support-vector classifier_ (SVC)\n","\n","Other models and datasets  ('mnist', 'iris', 'moon') can also be examined.\n","\n","### Qa Explain GridSearchCV\n","\n","There are two code cells below: 1) function setup, 2) the actual grid-search.\n","\n","Review the code cells and write a __short__ summary. Mainly focus on __cell 2__, but dig into cell 1 if you find it interesting (notice the use of local-function, a nifty feature in python).\n","  \n","In detail, examine the lines:  \n","  \n","```python\n","grid_tuned = GridSearchCV(model, tuning_parameters, ..\n","grid_tuned.fit(X_train, y_train)\n","..\n","FullReport(grid_tuned , X_test, y_test, time_gridsearch)\n","```\n","and write a short description of how the `GridSeachCV` works: explain how the search parameter set is created and the overall search mechanism is functioning (without going into too much detail).\n","\n","What role does the parameter `scoring='f1_micro'` play in the `GridSearchCV`, and what does `n_jobs=-1` mean?"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"GPA22G_4idLO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700221382378,"user_tz":-60,"elapsed":4520,"user":{"displayName":"Hashem abdou","userId":"07504651210053120960"}},"outputId":"b26951e5-90b3-4be4-8c4e-b59f13dae175"},"outputs":[{"output_type":"stream","name":"stdout","text":["OK(function setup)\n"]}],"source":["# TODO: Qa, code review..cell 1) function setup\n","\n","from time import time\n","import numpy as np\n","import sys\n","\n","from sklearn import svm\n","from sklearn.linear_model import SGDClassifier\n","\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n","from sklearn.metrics import classification_report, f1_score\n","from sklearn import datasets\n","\n","from libitmal import dataloaders as itmaldataloaders # Needed for load of iris, moon and mnist\n","\n","currmode=\"N/A\" # GLOBAL var!\n","\n","def SearchReport(model):\n","\n","    def GetBestModelCTOR(model, best_params):\n","        def GetParams(best_params):\n","            ret_str=\"\"\n","            for key in sorted(best_params):\n","                value = best_params[key]\n","                temp_str = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n","                if len(ret_str)>0:\n","                    ret_str += ','\n","                ret_str += f'{key}={temp_str}{value}{temp_str}'\n","            return ret_str\n","        try:\n","            param_str = GetParams(best_params)\n","            return type(model).__name__ + '(' + param_str + ')'\n","        except:\n","            return \"N/A(1)\"\n","\n","    print(\"\\nBest model set found on train set:\")\n","    print()\n","    print(f\"\\tbest parameters={model.best_params_}\")\n","    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n","    print(f\"\\tbest index={model.best_index_}\")\n","    print()\n","    print(f\"Best estimator CTOR:\")\n","    print(f\"\\t{model.best_estimator_}\")\n","    print()\n","    try:\n","        print(f\"Grid scores ('{model.scoring}') on development set:\")\n","        means = model.cv_results_['mean_test_score']\n","        stds  = model.cv_results_['std_test_score']\n","        i=0\n","        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n","            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n","            i += 1\n","    except:\n","        print(\"WARNING: the random search do not provide means/stds\")\n","\n","    global currmode\n","    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"\n","    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_\n","\n","def ClassificationReport(model, X_test, y_test, target_names=None):\n","    assert X_test.shape[0]==y_test.shape[0]\n","    print(\"\\nDetailed classification report:\")\n","    print(\"\\tThe model is trained on the full development set.\")\n","    print(\"\\tThe scores are computed on the full evaluation set.\")\n","    print()\n","    y_true, y_pred = y_test, model.predict(X_test)\n","    print(classification_report(y_true, y_pred, target_names=target_names))\n","    print()\n","\n","def FullReport(model, X_test, y_test, t):\n","    print(f\"SEARCH TIME: {t:0.2f} sec\")\n","    beststr, bestmodel = SearchReport(model)\n","    ClassificationReport(model, X_test, y_test)\n","    print(f\"CTOR for best model: {bestmodel}\\n\")\n","    print(f\"{beststr}\\n\")\n","    return beststr, bestmodel\n","\n","def LoadAndSetupData(mode, test_size=0.3):\n","    assert test_size>=0.0 and test_size<=1.0\n","\n","    def ShapeToString(Z):\n","        n = Z.ndim\n","        s = \"(\"\n","        for i in range(n):\n","            s += f\"{Z.shape[i]:5d}\"\n","            if i+1!=n:\n","                s += \";\"\n","        return s+\")\"\n","\n","    global currmode\n","    currmode=mode\n","    print(f\"DATA: {currmode}..\")\n","\n","    if mode=='moon':\n","        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n","        itmaldataloaders.MOON_Plot(X, y)\n","    elif mode=='mnist':\n","        X, y = itmaldataloaders.MNIST_GetDataSet(load_mode=0)\n","        if X.ndim==3:\n","            X=np.reshape(X, (X.shape[0], -1))\n","    elif mode=='iris':\n","        X, y = itmaldataloaders.IRIS_GetDataSet()\n","    else:\n","        raise ValueError(f\"could not load data for that particular mode='{mode}', only 'moon'/'mnist'/'iris' supported\")\n","\n","    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n","\n","    assert X.ndim==2\n","    assert X.shape[0]==y.shape[0]\n","    assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=test_size, random_state=0, shuffle=True\n","    )\n","\n","    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n","    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n","    print()\n","\n","    return X_train, X_test, y_train, y_test\n","\n","def TryKerasImport(verbose=True):\n","\n","    kerasok = True\n","    try:\n","        import keras as keras_try\n","    except:\n","        kerasok = False\n","\n","    tensorflowkerasok = True\n","    try:\n","        import tensorflow.keras as tensorflowkeras_try\n","    except:\n","        tensorflowkerasok = False\n","\n","    ok = kerasok or tensorflowkerasok\n","\n","    if not ok and verbose:\n","        if not kerasok:\n","            print(\"WARNING: importing 'keras' failed\", file=sys.stderr)\n","        if not tensorflowkerasok:\n","            print(\"WARNING: importing 'tensorflow.keras' failed\", file=sys.stderr)\n","\n","    return ok\n","\n","print(f\"OK(function setup\" + (\"\" if TryKerasImport() else \", hope MNIST loads works because it seems you miss the installation of Keras or Tensorflow!\") + \")\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"LRWOFUzaidLT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700221393429,"user_tz":-60,"elapsed":1726,"user":{"displayName":"Hashem abdou","userId":"07504651210053120960"}},"outputId":"ac98c719-d939-40e6-8981-5f7ee236b1b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["DATA: iris..\n","  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n","  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n","  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n","\n","SEARCH TIME: 1.33 sec\n","\n","Best model set found on train set:\n","\n","\tbest parameters={'C': 1, 'kernel': 'linear'}\n","\tbest 'f1_micro' score=0.9714285714285715\n","\tbest index=2\n","\n","Best estimator CTOR:\n","\tSVC(C=1, gamma=0.001, kernel='linear')\n","\n","Grid scores ('f1_micro') on development set:\n","\t[ 0]: 0.962 (+/-0.093) for {'C': 0.1, 'kernel': 'linear'}\n","\t[ 1]: 0.371 (+/-0.038) for {'C': 0.1, 'kernel': 'rbf'}\n","\t[ 2]: 0.971 (+/-0.047) for {'C': 1, 'kernel': 'linear'}\n","\t[ 3]: 0.695 (+/-0.047) for {'C': 1, 'kernel': 'rbf'}\n","\t[ 4]: 0.952 (+/-0.085) for {'C': 10, 'kernel': 'linear'}\n","\t[ 5]: 0.924 (+/-0.097) for {'C': 10, 'kernel': 'rbf'}\n","\n","Detailed classification report:\n","\tThe model is trained on the full development set.\n","\tThe scores are computed on the full evaluation set.\n","\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        16\n","           1       1.00      0.94      0.97        18\n","           2       0.92      1.00      0.96        11\n","\n","    accuracy                           0.98        45\n","   macro avg       0.97      0.98      0.98        45\n","weighted avg       0.98      0.98      0.98        45\n","\n","\n","CTOR for best model: SVC(C=1, gamma=0.001, kernel='linear')\n","\n","best: dat=iris, score=0.97143, model=SVC(C=1,kernel='linear')\n","\n","OK(grid-search)\n"]}],"source":["# TODO: Qa, code review..cell 2) the actual grid-search\n","\n","# Setup data\n","X_train, X_test, y_train, y_test = LoadAndSetupData(\n","    'iris')  # 'iris', 'moon', or 'mnist'\n","\n","# Setup search parameters\n","model = svm.SVC(\n","    gamma=0.001\n",")  # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks,\n","# FIX:  replace with model = svm.SVC(gamma=0.001)\n","\n","tuning_parameters = {\n","    'kernel': ('linear', 'rbf'),\n","    'C': [0.1, 1, 10]\n","}\n","\n","CV = 5\n","VERBOSE = 0\n","\n","# Run GridSearchCV for the model\n","grid_tuned = GridSearchCV(model,\n","                          tuning_parameters,\n","                          cv=CV,\n","                          scoring='f1_micro',\n","                          verbose=VERBOSE,\n","                          n_jobs=-1)\n","\n","start = time()\n","grid_tuned.fit(X_train, y_train)\n","t = time() - start\n","\n","# Report result\n","b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n","print('OK(grid-search)')"]},{"cell_type":"markdown","metadata":{"id":"TWvGkdM3idLV"},"source":["### Qb Hyperparameter Grid Search using an SDG classifier\n","\n","Now, replace the `svm.SVC` model with an `SGDClassifier` and a suitable set of the hyperparameters for that model.\n","\n","You need at least four or five different hyperparameters from the `SGDClassifier` in the search-space before it begins to take considerable compute time doing the full grid search.\n","\n","So, repeat the search with the `SGDClassifier`, and be sure to add enough hyperparameters to the grid-search, such that the search takes a considerable time to run, that is a couple of minutes or up to some hours.."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"HXDZ0dywidLX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700224915816,"user_tz":-60,"elapsed":6140,"user":{"displayName":"Hashem abdou","userId":"07504651210053120960"}},"outputId":"583c0b74-614c-4f4a-a3fd-6a4831caf401"},"outputs":[{"output_type":"stream","name":"stdout","text":["DATA: iris..\n","  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n","  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n","  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n","2025 fits failed out of a total of 2700.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","2025 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 892, in fit\n","    self._more_validate_params()\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 149, in _more_validate_params\n","    raise ValueError(\"eta0 must be > 0\")\n","ValueError: eta0 must be > 0\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan 0.80952381 0.83809524 0.82857143\n"," 0.83809524 0.81904762 0.75238095 0.8        0.82857143 0.80952381\n"," 0.79047619 0.83809524 0.79047619 0.85714286 0.76190476 0.78095238\n"," 0.7047619  0.80952381 0.73333333 0.87619048 0.6952381  0.72380952\n"," 0.80952381 0.8952381  0.8952381  0.82857143 0.81904762 0.74285714\n"," 0.78095238 0.74285714 0.93333333 0.8952381  0.85714286 0.87619048\n"," 0.8        0.81904762 0.87619048 0.84761905 0.88571429 0.74285714\n"," 0.83809524 0.74285714 0.9047619  0.85714286 0.82857143 0.73333333\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan 0.9047619  0.78095238 0.81904762\n"," 0.95238095 0.82857143 0.79047619 0.93333333 0.84761905 0.84761905\n"," 0.9047619  0.79047619 0.8952381  0.95238095 0.88571429 0.74285714\n"," 0.86666667 0.82857143 0.86666667 0.9047619  0.87619048 0.82857143\n"," 0.94285714 0.92380952 0.83809524 0.91428571 0.79047619 0.86666667\n"," 0.9047619  0.78095238 0.8952381  0.95238095 0.83809524 0.87619048\n"," 0.93333333 0.93333333 0.8        0.94285714 0.80952381 0.83809524\n"," 0.93333333 0.85714286 0.82857143 0.97142857 0.80952381 0.86666667\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan 0.96190476 0.82857143 0.96190476\n"," 0.98095238 0.8952381  0.86666667 0.95238095 0.84761905 0.93333333\n"," 0.98095238 0.93333333 0.88571429 0.95238095 0.9047619  0.95238095\n"," 0.94285714 0.88571429 0.9047619  0.97142857 0.8952381  0.94285714\n"," 0.97142857 0.80952381 0.94285714 0.98095238 0.86666667 0.8952381\n"," 0.95238095 0.93333333 0.92380952 0.98095238 0.93333333 0.84761905\n"," 0.95238095 0.92380952 0.94285714 0.92380952 0.86666667 0.95238095\n"," 0.96190476 0.87619048 0.96190476 0.97142857 0.91428571 0.86666667\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan]\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["SEARCH TIME: 5.65 sec\n","\n","Best model set found on train set:\n","\n","\tbest parameters={'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\tbest 'f1_micro' score=0.980952380952381\n","\tbest index=408\n","\n","Best estimator CTOR:\n","\tSGDClassifier(alpha=0.01, penalty='l1')\n","\n","Grid scores ('f1_micro') on development set:\n","\t[ 0]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[ 1]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[ 2]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[ 3]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[ 4]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[ 5]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[ 6]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[ 7]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[ 8]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[ 9]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[10]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[11]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[12]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[13]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[14]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[15]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[16]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[17]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[18]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[19]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[20]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[21]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[22]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[23]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[24]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[25]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[26]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[27]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[28]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[29]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[30]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[31]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[32]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[33]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[34]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[35]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[36]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[37]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[38]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[39]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[40]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[41]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[42]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[43]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[44]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[45]: 0.810 (+/-0.217) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[46]: 0.838 (+/-0.245) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[47]: 0.829 (+/-0.166) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[48]: 0.838 (+/-0.177) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[49]: 0.819 (+/-0.258) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[50]: 0.752 (+/-0.111) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[51]: 0.800 (+/-0.194) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[52]: 0.829 (+/-0.129) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[53]: 0.810 (+/-0.170) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[54]: 0.790 (+/-0.230) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[55]: 0.838 (+/-0.222) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[56]: 0.790 (+/-0.311) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[57]: 0.857 (+/-0.217) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[58]: 0.762 (+/-0.148) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[59]: 0.781 (+/-0.286) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[60]: 0.705 (+/-0.327) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[61]: 0.810 (+/-0.200) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[62]: 0.733 (+/-0.177) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[63]: 0.876 (+/-0.230) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[64]: 0.695 (+/-0.129) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[65]: 0.724 (+/-0.194) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[66]: 0.810 (+/-0.104) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[67]: 0.895 (+/-0.071) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[68]: 0.895 (+/-0.164) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[69]: 0.829 (+/-0.245) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[70]: 0.819 (+/-0.194) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[71]: 0.743 (+/-0.114) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[72]: 0.781 (+/-0.230) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[73]: 0.743 (+/-0.316) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[74]: 0.933 (+/-0.076) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[75]: 0.895 (+/-0.236) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[76]: 0.857 (+/-0.104) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[77]: 0.876 (+/-0.205) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[78]: 0.800 (+/-0.291) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[79]: 0.819 (+/-0.185) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[80]: 0.876 (+/-0.166) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[81]: 0.848 (+/-0.265) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[82]: 0.886 (+/-0.230) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[83]: 0.743 (+/-0.453) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[84]: 0.838 (+/-0.230) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[85]: 0.743 (+/-0.155) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[86]: 0.905 (+/-0.209) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[87]: 0.857 (+/-0.190) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[88]: 0.829 (+/-0.273) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[89]: 0.733 (+/-0.453) for {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[90]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[91]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[92]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[93]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[94]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[95]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[96]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[97]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[98]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[99]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[100]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[101]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[102]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[103]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[104]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[105]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[106]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[107]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[108]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[109]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[110]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[111]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[112]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[113]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[114]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[115]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[116]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[117]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[118]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[119]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[120]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[121]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[122]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[123]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[124]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[125]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[126]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[127]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[128]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[129]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[130]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[131]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[132]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[133]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[134]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[135]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[136]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[137]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[138]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[139]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[140]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[141]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[142]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[143]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[144]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[145]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[146]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[147]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[148]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[149]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[150]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[151]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[152]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[153]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[154]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[155]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[156]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[157]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[158]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[159]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[160]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[161]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[162]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[163]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[164]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[165]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[166]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[167]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[168]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[169]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[170]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[171]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[172]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[173]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[174]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[175]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[176]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[177]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[178]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[179]: nan (+/-nan) for {'alpha': 0.0001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[180]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[181]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[182]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[183]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[184]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[185]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[186]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[187]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[188]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[189]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[190]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[191]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[192]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[193]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[194]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[195]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[196]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[197]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[198]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[199]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[200]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[201]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[202]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[203]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[204]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[205]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[206]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[207]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[208]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[209]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[210]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[211]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[212]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[213]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[214]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[215]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[216]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[217]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[218]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[219]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[220]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[221]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[222]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[223]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[224]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[225]: 0.905 (+/-0.085) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[226]: 0.781 (+/-0.187) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[227]: 0.819 (+/-0.410) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[228]: 0.952 (+/-0.085) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[229]: 0.829 (+/-0.166) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[230]: 0.790 (+/-0.245) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[231]: 0.933 (+/-0.076) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[232]: 0.848 (+/-0.185) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[233]: 0.848 (+/-0.185) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[234]: 0.905 (+/-0.159) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[235]: 0.790 (+/-0.205) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[236]: 0.895 (+/-0.203) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[237]: 0.952 (+/-0.104) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[238]: 0.886 (+/-0.097) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[239]: 0.743 (+/-0.299) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[240]: 0.867 (+/-0.251) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[241]: 0.829 (+/-0.177) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[242]: 0.867 (+/-0.185) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[243]: 0.905 (+/-0.148) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[244]: 0.876 (+/-0.129) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[245]: 0.829 (+/-0.230) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[246]: 0.943 (+/-0.093) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[247]: 0.924 (+/-0.196) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[248]: 0.838 (+/-0.253) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[249]: 0.914 (+/-0.071) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[250]: 0.790 (+/-0.267) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[251]: 0.867 (+/-0.185) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[252]: 0.905 (+/-0.200) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[253]: 0.781 (+/-0.205) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[254]: 0.895 (+/-0.194) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[255]: 0.952 (+/-0.060) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[256]: 0.838 (+/-0.293) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[257]: 0.876 (+/-0.205) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[258]: 0.933 (+/-0.076) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[259]: 0.933 (+/-0.076) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[260]: 0.800 (+/-0.229) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[261]: 0.943 (+/-0.093) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[262]: 0.810 (+/-0.181) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[263]: 0.838 (+/-0.293) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[264]: 0.933 (+/-0.114) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[265]: 0.857 (+/-0.181) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[266]: 0.829 (+/-0.238) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[267]: 0.971 (+/-0.076) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[268]: 0.810 (+/-0.256) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[269]: 0.867 (+/-0.164) for {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[270]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[271]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[272]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[273]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[274]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[275]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[276]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[277]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[278]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[279]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[280]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[281]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[282]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[283]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[284]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[285]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[286]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[287]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[288]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[289]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[290]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[291]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[292]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[293]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[294]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[295]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[296]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[297]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[298]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[299]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[300]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[301]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[302]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[303]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[304]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[305]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[306]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[307]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[308]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[309]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[310]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[311]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[312]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[313]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[314]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[315]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[316]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[317]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[318]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[319]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[320]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[321]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[322]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[323]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[324]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[325]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[326]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[327]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[328]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[329]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[330]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[331]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[332]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[333]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[334]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[335]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[336]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[337]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[338]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[339]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[340]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[341]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[342]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[343]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[344]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[345]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[346]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[347]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[348]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[349]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[350]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[351]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[352]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[353]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[354]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[355]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[356]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[357]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[358]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[359]: nan (+/-nan) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[360]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[361]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[362]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[363]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[364]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[365]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[366]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[367]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[368]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[369]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[370]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[371]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[372]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[373]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[374]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[375]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[376]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[377]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[378]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[379]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[380]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[381]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[382]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[383]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[384]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[385]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[386]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[387]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[388]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[389]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[390]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[391]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[392]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[393]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[394]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[395]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[396]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[397]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[398]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[399]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[400]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[401]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[402]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[403]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[404]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[405]: 0.962 (+/-0.071) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[406]: 0.829 (+/-0.114) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[407]: 0.962 (+/-0.071) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[408]: 0.981 (+/-0.047) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[409]: 0.895 (+/-0.229) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[410]: 0.867 (+/-0.258) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[411]: 0.952 (+/-0.085) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[412]: 0.848 (+/-0.194) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[413]: 0.933 (+/-0.047) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[414]: 0.981 (+/-0.047) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[415]: 0.933 (+/-0.097) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[416]: 0.886 (+/-0.143) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[417]: 0.952 (+/-0.060) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[418]: 0.905 (+/-0.120) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[419]: 0.952 (+/-0.104) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[420]: 0.943 (+/-0.093) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[421]: 0.886 (+/-0.238) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[422]: 0.905 (+/-0.135) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[423]: 0.971 (+/-0.076) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[424]: 0.895 (+/-0.164) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[425]: 0.943 (+/-0.071) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[426]: 0.971 (+/-0.076) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[427]: 0.810 (+/-0.159) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[428]: 0.943 (+/-0.093) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[429]: 0.981 (+/-0.047) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[430]: 0.867 (+/-0.093) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[431]: 0.895 (+/-0.111) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[432]: 0.952 (+/-0.085) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[433]: 0.933 (+/-0.047) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[434]: 0.924 (+/-0.076) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[435]: 0.981 (+/-0.047) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[436]: 0.933 (+/-0.143) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[437]: 0.848 (+/-0.251) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[438]: 0.952 (+/-0.060) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[439]: 0.924 (+/-0.097) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[440]: 0.943 (+/-0.071) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[441]: 0.924 (+/-0.129) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[442]: 0.867 (+/-0.152) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[443]: 0.952 (+/-0.120) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[444]: 0.962 (+/-0.071) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[445]: 0.876 (+/-0.205) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[446]: 0.962 (+/-0.071) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[447]: 0.971 (+/-0.076) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[448]: 0.914 (+/-0.212) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[449]: 0.867 (+/-0.164) for {'alpha': 0.01, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[450]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[451]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[452]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[453]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[454]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[455]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[456]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[457]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[458]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[459]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[460]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[461]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[462]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[463]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[464]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[465]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[466]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[467]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[468]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[469]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[470]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[471]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[472]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[473]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[474]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[475]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[476]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[477]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[478]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[479]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[480]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[481]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[482]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[483]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[484]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[485]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[486]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[487]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[488]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[489]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[490]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[491]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[492]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[493]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[494]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[495]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[496]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[497]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[498]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[499]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[500]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[501]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[502]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[503]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[504]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1'}\n","\t[505]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 100, 'penalty': 'l2'}\n","\t[506]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[507]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[508]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[509]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[510]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[511]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[512]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[513]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l1'}\n","\t[514]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'l2'}\n","\t[515]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[516]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[517]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[518]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[519]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[520]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[521]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[522]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n","\t[523]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n","\t[524]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[525]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[526]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[527]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[528]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[529]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[530]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\t[531]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l1'}\n","\t[532]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n","\t[533]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'elasticnet'}\n","\t[534]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l1'}\n","\t[535]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'l2'}\n","\t[536]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': 'elasticnet'}\n","\t[537]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l1'}\n","\t[538]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'l2'}\n","\t[539]: nan (+/-nan) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n","\n","Detailed classification report:\n","\tThe model is trained on the full development set.\n","\tThe scores are computed on the full evaluation set.\n","\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        16\n","           1       1.00      0.89      0.94        18\n","           2       0.85      1.00      0.92        11\n","\n","    accuracy                           0.96        45\n","   macro avg       0.95      0.96      0.95        45\n","weighted avg       0.96      0.96      0.96        45\n","\n","\n","CTOR for best model: SGDClassifier(alpha=0.01, penalty='l1')\n","\n","best: dat=iris, score=0.98095, model=SGDClassifier(alpha=0.01,learning_rate='optimal',loss='hinge',max_iter=1000,penalty='l1')\n","\n","OK(grid-search)\n"]}],"source":["# Setup data\n","X_train, X_test, y_train, y_test = LoadAndSetupData(\n","    'iris')  # 'iris', 'moon', or 'mnist'\n","#setup search parameters\n","model = SGDClassifier()\n","\n","tuning_parameters = {\n","    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n","    'penalty': ['l1', 'l2', 'elasticnet'],\n","    'alpha': [0.0001, 0.001, 0.01],\n","    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n","    'max_iter': [100, 1000, 10000]\n","}\n","\n","CV = 5\n","VERBOSE = 0\n","\n","# Run GridSearchCV for the model\n","SGD_grid_tuned = GridSearchCV(model,\n","                          tuning_parameters,\n","                          cv=CV,\n","                          scoring='f1_micro',\n","                          verbose=VERBOSE,\n","                          n_jobs=-1)\n","\n","start = time()\n","SGD_grid_tuned.fit(X_train, y_train)\n","t = time() - start\n","\n","# Report result\n","b0, m0 = FullReport(SGD_grid_tuned, X_test, y_test, t)\n","print('OK(grid-search)')"]},{"cell_type":"markdown","metadata":{"id":"fRjljcl-idLa"},"source":["### Qc Hyperparameter Random  Search using an SDG classifier\n","\n","Now, add code to run a `RandomizedSearchCV` instead.\n","\n","<img src=\"https://itundervisning.ase.au.dk/SWMAL/L09/Figs/randomsearch.png\" alt=\"WARNING: could not get image from server.\"  style=\"width:350px\" >\n","<small><em>\n","    <center> Conceptual graphical view of randomized search for two distinct hyperparameters. </center>\n","</em></small>\n","\n","Use these default parameters for the random search, similar to the default parameters for the grid search\n","\n","```python\n","random_tuned = RandomizedSearchCV(\n","    model,\n","    tuning_parameters,\n","    n_iter=20,\n","    random_state=42,\n","    cv=CV,\n","    scoring='f1_micro',\n","    verbose=VERBOSE,\n","    n_jobs=-1\n",")\n","```\n","\n","but with the two new parameters, `n_iter` and `random_state` added. Since the search-type is now random, the `random_state` gives sense, but essential to random search is the new `n_tier` parameter.\n","\n","So: investigate the `n_iter` parameter...in code and write a conceptual explanation  in text.\n","\n","Comparison of time (seconds) to complete `GridSearch` versus `RandomizedSearchCV`, does not necessarily give any sense, if your grid search completes in a few seconds (as for the iris tiny-data). You need a search that runs for minutes, hours, or days.\n","\n","But you could compare the best-tuned parameter set and best scoring for the two methods. Is the random search best model close to the grid search?  "]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Rt6kmvx8idLc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700226406752,"user_tz":-60,"elapsed":520,"user":{"displayName":"Hashem abdou","userId":"07504651210053120960"}},"outputId":"8dee5b32-3d67-4126-f303-41c05e41b551"},"outputs":[{"output_type":"stream","name":"stdout","text":["DATA: iris..\n","  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n","  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n","  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n","\n","SEARCH TIME: 0.26 sec\n","\n","Best model set found on train set:\n","\n","\tbest parameters={'penalty': 'l1', 'max_iter': 1000, 'loss': 'log', 'learning_rate': 'optimal', 'alpha': 0.01}\n","\tbest 'f1_micro' score=0.9714285714285715\n","\tbest index=17\n","\n","Best estimator CTOR:\n","\tSGDClassifier(alpha=0.01, loss='log', penalty='l1')\n","\n","Grid scores ('f1_micro') on development set:\n","\t[ 0]: nan (+/-nan) for {'penalty': 'l1', 'max_iter': 100, 'loss': 'squared_error', 'learning_rate': 'invscaling', 'alpha': 0.01}\n","\t[ 1]: nan (+/-nan) for {'penalty': 'l1', 'max_iter': 1000, 'loss': 'modified_huber', 'learning_rate': 'invscaling', 'alpha': 0.01}\n","\t[ 2]: nan (+/-nan) for {'penalty': 'l1', 'max_iter': 100, 'loss': 'squared_hinge', 'learning_rate': 'constant', 'alpha': 0.001}\n","\t[ 3]: nan (+/-nan) for {'penalty': 'l2', 'max_iter': 1000, 'loss': 'modified_huber', 'learning_rate': 'invscaling', 'alpha': 0.01}\n","\t[ 4]: 0.829 (+/-0.196) for {'penalty': 'l2', 'max_iter': 1000, 'loss': 'squared_hinge', 'learning_rate': 'optimal', 'alpha': 0.01}\n","\t[ 5]: nan (+/-nan) for {'penalty': 'l1', 'max_iter': 1000, 'loss': 'hinge', 'learning_rate': 'invscaling', 'alpha': 0.001}\n","\t[ 6]: nan (+/-nan) for {'penalty': 'l2', 'max_iter': 1000, 'loss': 'huber', 'learning_rate': 'constant', 'alpha': 0.001}\n","\t[ 7]: 0.724 (+/-0.093) for {'penalty': 'elasticnet', 'max_iter': 1000, 'loss': 'huber', 'learning_rate': 'optimal', 'alpha': 0.01}\n","\t[ 8]: nan (+/-nan) for {'penalty': 'l1', 'max_iter': 10000, 'loss': 'log', 'learning_rate': 'constant', 'alpha': 0.01}\n","\t[ 9]: nan (+/-nan) for {'penalty': 'l1', 'max_iter': 10000, 'loss': 'squared_epsilon_insensitive', 'learning_rate': 'constant', 'alpha': 0.001}\n","\t[10]: 0.819 (+/-0.175) for {'penalty': 'l2', 'max_iter': 100, 'loss': 'modified_huber', 'learning_rate': 'optimal', 'alpha': 0.0001}\n","\t[11]: nan (+/-nan) for {'penalty': 'l2', 'max_iter': 1000, 'loss': 'modified_huber', 'learning_rate': 'invscaling', 'alpha': 0.0001}\n","\t[12]: 0.905 (+/-0.104) for {'penalty': 'l1', 'max_iter': 1000, 'loss': 'hinge', 'learning_rate': 'optimal', 'alpha': 0.001}\n","\t[13]: nan (+/-nan) for {'penalty': 'l1', 'max_iter': 10000, 'loss': 'hinge', 'learning_rate': 'constant', 'alpha': 0.001}\n","\t[14]: nan (+/-nan) for {'penalty': 'l1', 'max_iter': 10000, 'loss': 'squared_error', 'learning_rate': 'constant', 'alpha': 0.01}\n","\t[15]: nan (+/-nan) for {'penalty': 'l1', 'max_iter': 100, 'loss': 'squared_epsilon_insensitive', 'learning_rate': 'invscaling', 'alpha': 0.0001}\n","\t[16]: 0.343 (+/-0.071) for {'penalty': 'l2', 'max_iter': 100, 'loss': 'squared_epsilon_insensitive', 'learning_rate': 'optimal', 'alpha': 0.001}\n","\t[17]: 0.971 (+/-0.047) for {'penalty': 'l1', 'max_iter': 1000, 'loss': 'log', 'learning_rate': 'optimal', 'alpha': 0.01}\n","\t[18]: nan (+/-nan) for {'penalty': 'l1', 'max_iter': 1000, 'loss': 'squared_epsilon_insensitive', 'learning_rate': 'invscaling', 'alpha': 0.01}\n","\t[19]: nan (+/-nan) for {'penalty': 'elasticnet', 'max_iter': 1000, 'loss': 'hinge', 'learning_rate': 'invscaling', 'alpha': 0.0001}\n","\n","Detailed classification report:\n","\tThe model is trained on the full development set.\n","\tThe scores are computed on the full evaluation set.\n","\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        34\n","           1       1.00      0.94      0.97        32\n","           2       0.95      1.00      0.97        39\n","\n","    accuracy                           0.98       105\n","   macro avg       0.98      0.98      0.98       105\n","weighted avg       0.98      0.98      0.98       105\n","\n","\n","CTOR for best model: SGDClassifier(alpha=0.01, loss='log', penalty='l1')\n","\n","best: dat=iris, score=0.97143, model=SGDClassifier(alpha=0.01,learning_rate='optimal',loss='log',max_iter=1000,penalty='l1')\n","\n","OK(grid-search)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n","70 fits failed out of a total of 100.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","70 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 892, in fit\n","    self._more_validate_params()\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 149, in _more_validate_params\n","    raise ValueError(\"eta0 must be > 0\")\n","ValueError: eta0 must be > 0\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.82857143        nan\n","        nan 0.72380952        nan        nan 0.81904762        nan\n"," 0.9047619         nan        nan        nan 0.34285714 0.97142857\n","        nan        nan]\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n","  warnings.warn(\n"]}],"source":["# TODO:\n","X_train, X_test, y_train, y_test = LoadAndSetupData(\n","    'iris')  # 'iris', 'moon', or 'mnist'\n","#setup search parameters\n","model = SGDClassifier()\n","\n","tuning_parameters = {\n","    'loss': ['hinge', 'log', 'modified_huber', 'squared_epsilon_insensitive',\n","             'squared_error', 'squared_hinge', 'huber'],\n","    'penalty': ['l1', 'l2', 'elasticnet'],\n","    'alpha': [0.0001, 0.001, 0.01],\n","    'learning_rate': ['constant', 'optimal', 'invscaling'],\n","    'max_iter': [100, 1000, 10000]\n","}\n","CV = 5\n","VERBOSE = 0\n","\n","random_tuned = RandomizedSearchCV(\n","    model,\n","    tuning_parameters,\n","    n_iter=20,\n","    random_state=42,\n","    cv=CV,\n","    scoring='f1_micro',\n","    verbose=VERBOSE,\n","    n_jobs=-1\n",")\n","\n","start = time()\n","random_tuned.fit(X_train, y_train)\n","t = time() - start\n","\n","# Report result\n","b0, m0 = FullReport(random_tuned,X_train,y_train,t)\n","print('OK(grid-search)')"]},{"cell_type":"markdown","metadata":{"id":"54Yf9QHbidLe"},"source":["## Qd MNIST Search Quest II\n","\n","Finally, a search-quest competition: __who can find the best model+hyperparameters for the MNIST dataset?__\n","\n","You change to the MNIST data by calling `LoadAndSetupData('mnist')`, and this is a completely other ball-game that the iris _tiny-data_: it's much larger (but still far from _big-data_)!\n","\n","* You might opt for the exhaustive grid search, or use the faster but-less optimal random search...your choice.\n","\n","* You are free to pick any classifier in Scikit-learn, even algorithms we have not discussed yet---__except Neural Networks and KNeighborsClassifier!__.\n","\n","* Keep the score function at `f1_micro`, otherwise, we will be comparing 'æbler og pærer'.\n","\n","* And, you may also want to scale your input data for some models to perform better.\n","\n","* __REMEMBER__, DO NOT USE any Neural Network models. This also means not to use any `Keras` or `Tensorflow` models...since they outperform most other models, and there are also too many examples on the internet to cut-and-paste from!\n","\n","Check your result by printing the first _return_ value from `FullReport()`\n","```python\n","b1, m1 = FullReport(random_tuned , X_test, y_test, time_randomsearch)\n","print(b1)\n","```\n","that will display a result like\n","```\n","best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n","```\n","and paste your currently best model into the message box, for ITMAL group 09 like\n","```\n","Grp09: best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n","\n","Grp09: CTOR for best model: SGDClassifier(alpha=1.0, average=False, class_weight=None, early_stopping=False,\n","              epsilon=0.1, eta0=0.0001, fit_intercept=True, l1_ratio=0.15,\n","              learning_rate='invscaling', loss='hinge', max_iter=1000,\n","              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n","              random_state=None, shuffle=True, tol=0.001,\n","              validation_fraction=0.1, verbose=0, warm_start=False)\n","```\n","              \n","on Brightspace: \"L09: Regularisering, optimering og søgning\" | \"Qd MNIST Search Quest\"\n","\n","> https://itundervisning.ase.au.dk/itmal_quest/index.php\n","\n","and, check if your score (for MNIST) is better than the currently best score. Republish if you get a better score than your own previously best. Deadline for submission of scores is the same as the deadline for the O3 journal handin.\n","\n","Remember to provide an ITMAL group name manually, so we can identify a winner: the 1. st price is  cake!\n","\n","For the journal hand-in, report your progress in scoring choosing different models, hyperparameters to search and how you might need to preprocess your data...and note, that the journal will not be accepted unless it contains information about Your results published on the Brightspace 'Search Quest II' page!"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"9QkyZ_zZidLi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700230145965,"user_tz":-60,"elapsed":1117263,"user":{"displayName":"Hashem abdou","userId":"07504651210053120960"}},"outputId":"e8560b3e-a9f4-48bb-c096-3c27e61110c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["DATA: mnist..\n","  org. data:  X.shape      =(70000;  784), y.shape      =(70000)\n","  train data: X_train.shape=(49000;  784), y_train.shape=(49000)\n","  test data:  X_test.shape =(21000;  784), y_test.shape =(21000)\n","\n","SEARCH TIME: 1078.87 sec\n","\n","Best model set found on train set:\n","\n","\tbest parameters={'C': 0.1, 'kernel': 'poly'}\n","\tbest 'f1_micro' score=0.9514999999999999\n","\tbest index=2\n","\n","Best estimator CTOR:\n","\tSVC(C=0.1, gamma=0.001, kernel='poly')\n","\n","Grid scores ('f1_micro') on development set:\n","\t[ 0]: 0.905 (+/-0.006) for {'C': 0.1, 'kernel': 'linear'}\n","\t[ 1]: 0.114 (+/-0.000) for {'C': 0.1, 'kernel': 'rbf'}\n","\t[ 2]: 0.951 (+/-0.004) for {'C': 0.1, 'kernel': 'poly'}\n","\t[ 3]: 0.905 (+/-0.006) for {'C': 1, 'kernel': 'linear'}\n","\t[ 4]: 0.114 (+/-0.000) for {'C': 1, 'kernel': 'rbf'}\n","\t[ 5]: 0.951 (+/-0.004) for {'C': 1, 'kernel': 'poly'}\n","\t[ 6]: 0.905 (+/-0.006) for {'C': 10, 'kernel': 'linear'}\n","\t[ 7]: 0.114 (+/-0.000) for {'C': 10, 'kernel': 'rbf'}\n","\t[ 8]: 0.951 (+/-0.004) for {'C': 10, 'kernel': 'poly'}\n","\n","Detailed classification report:\n","\tThe model is trained on the full development set.\n","\tThe scores are computed on the full evaluation set.\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.99      0.98      4826\n","           1       0.96      0.98      0.97      5492\n","           2       0.96      0.96      0.96      4875\n","           3       0.97      0.95      0.96      5024\n","           4       0.97      0.97      0.97      4820\n","           5       0.96      0.96      0.96      4413\n","           6       0.98      0.98      0.98      4831\n","           7       0.97      0.97      0.97      5104\n","           8       0.97      0.95      0.96      4783\n","           9       0.96      0.95      0.96      4832\n","\n","    accuracy                           0.97     49000\n","   macro avg       0.97      0.97      0.97     49000\n","weighted avg       0.97      0.97      0.97     49000\n","\n","\n","CTOR for best model: SVC(C=0.1, gamma=0.001, kernel='poly')\n","\n","best: dat=mnist, score=0.95150, model=SVC(C=0.1,kernel='poly')\n","\n","OK(randomized-search MNIST with Search time :1078.87 sec)\n"]}],"source":["# TODO:(in code and text..)\n","# TODO: Perform a RandomizedSearchCV on the MNIST dataset with SGDClassifier\n","X_train, X_test, y_train, y_test = LoadAndSetupData('mnist')\n","\n","# Setup search parameters\n","model_mnist = svm.SVC(\n","    gamma= 0.001\n",")\n","\n","tuning_parameters_mnist = {\n","    'kernel': ('linear', 'rbf','poly'),\n","    'C': [0.1, 1, 10]\n","}\n","\n","CV = 5\n","VERBOSE = 0\n","\n","tuned_mnist = GridSearchCV(\n","    model_mnist,\n","    tuning_parameters_mnist,\n","    cv=CV,\n","    scoring='f1_micro',\n","    verbose=VERBOSE,\n","    n_jobs=-1,\n",")\n","\n","start_mnist = time()\n","tuned_mnist.fit(X_train[:10000], y_train[:10000])\n","t = time() - start_mnist\n","# Report result\n","b_mnist, m_mnist = FullReport(tuned_mnist,X_train,y_train,t)\n","print(f\"OK(randomized-search MNIST with Search time :{t:0.2f} sec)\")\n"]},{"cell_type":"markdown","metadata":{"id":"hAlyc-ZAidLk"},"source":["REVISIONS||\n",":-|:-\n","2018-03-01| CEF, initial.\n","2018-03-05| CEF, updated.\n","2018-03-06| CEF, updated and spell checked.\n","2018-03-06| CEF, major overhaul of functions.\n","2018-03-06| CEF, fixed problem with MNIST load and Keras.\n","2018-03-07| CEF, modified report functions and changed Qc+d.\n","2018-03-11| CEF, updated Qd.\n","2018-03-12| CEF, added grid and random search figs and added bullets to Qd.\n","2018-03-13| CEF, fixed SVC and gamma issue, and changed dataload to be in fetchmode (non-keras).\n","2019-10-15| CEF, updated for ITMAL E19\n","2019-10-19| CEF, minor text update.\n","2019-10-23| CEF, changed demo model i Qd) from MLPClassifier to SVC.\n","2020-03-14| CEF, updated to ITMAL F20.\n","2020-10-20| CEF, updated to ITMAL E20.\n","2020-10-27| CEF, type fixes and minor update.\n","2020-10-28| CEF, added extra journal hand-in specs for Search Quest II, Qd.\n","2020-10-30| CEF, added non-use of KNeighborsClassifier to Search Quest II, Qd.\n","2020-11-19| CEF, changed load_mode=2 (Keras) to load_mode=0 (auto) for MNIST loader.\n","2021-03-17| CEF, updated to ITMAL F21.\n","2021-10-31| CEF, updated to ITMAL E21.\n","2021-11-05| CEF, removed iid=True paramter from GridSearchCV(), not present in current version of Scikit-learn (0.24.1).\n","2022-03-31| CEF, updated to SWMAL F22.\n","2022-08-30| CEF, updating to v1 changes.\n","2022-11-04| CEF, updated link to Brightspace, Search Quest II.\n","2022-11-04| CEF, fixed error \"TypeError: classification_report() takes 2 position..\".\n","2022-11-11| CEF, elaborated on Search Quest II deadline.\n","2023-03-24| CEF, updated link and updated to SWMAL F23."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":true},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}